{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "automatic-christian",
   "metadata": {},
   "source": [
    "# Zillow Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "general-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time, os\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "from fake_useragent import UserAgent\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-destination",
   "metadata": {},
   "source": [
    "### Approach #1 - Scrape All Property Links and and Download Individual Page HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nearby-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull all links via zipcodes and iterate through individual property links\n",
    "link_list = []\n",
    "\n",
    "def get_all_links(link, zipcode):\n",
    "    \n",
    "    #Create list of links\n",
    "    def get_page_links(search_page):\n",
    "        for div in search_page.findAll('div', attrs={'class' : 'list-card-info'}):\n",
    "            link_list.append(div.a['href'])\n",
    "        return link_list\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--user-agent=New User Agent\")\n",
    "    driver = webdriver.Chrome(chrome_path,options=options)\n",
    "    zipcode_specific_link = link[:-1] + '-{zc}/'.format(zc=zipcode)\n",
    "    driver.get(zipcode_specific_link)\n",
    "    first_soup = BeautifulSoup(driver.page_source)\n",
    "    total_listings = int(first_soup.find('div', attrs={'class' : 'total-text'}).text.replace(',', ''))\n",
    "    total_pages = total_listings//40\n",
    "    total_pages = total_pages + 2\n",
    "    for i in range(1,total_pages):\n",
    "        if (i == 1):\n",
    "            link_list = get_page_links(first_soup)\n",
    "            print('On page', i, '& Scraped', len(link_list), 'links so far')\n",
    "            time.sleep(random.uniform(3,6))\n",
    "        else:\n",
    "            if (i % 2 == 0):\n",
    "                driver.quit()\n",
    "                options = webdriver.ChromeOptions()\n",
    "                user_agent = ua.random\n",
    "                options.add_argument(f'user-agent={user_agent}')\n",
    "                options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "                driver = webdriver.Chrome(chrome_path,options=options)\n",
    "            if (i % 5 == 0):\n",
    "                time.sleep(random.uniform(300,320))\n",
    "            iterable_link = link[:-1] + '-{zc}/'.format(zc=zipcode) + '{pn}_p/'.format(pn=i)\n",
    "            driver.get(iterable_link)\n",
    "            page_soup = BeautifulSoup(driver.page_source)\n",
    "            link_list = get_page_links(page_soup)\n",
    "            print('On page', i, '& Scraped', len(link_list), 'links so far')\n",
    "            time.sleep(random.uniform(3,6))\n",
    "    driver.quit()\n",
    "    return print('All complete')\n",
    "\n",
    "# Apply to all zipcodes\n",
    "def zipcode_apply(link,zipcode_list):\n",
    "    for zipcode in zipcode_list:\n",
    "        print('Starting', zipcode)\n",
    "        get_all_links(link,zipcode)\n",
    "    return print('Done!')\n",
    "\n",
    "#Pickle final list\n",
    "with open('philly_property_lists', 'wb') as philly_links:\n",
    "    pickle.dump(link_list, philly_links)\n",
    "    \n",
    "#Iterate through properties and download HTML\n",
    "def download_properties_html(link_list):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--user-agent=New User Agent\")\n",
    "    driver = webdriver.Chrome(chrome_path,options=options) \n",
    "    for count,link in enumerate(link_list, start=1):\n",
    "        time.sleep(random.uniform(20.25, 40.75))\n",
    "        if (count % random.randint(2,8) == 0):\n",
    "            time.sleep(random.uniform(30,60))\n",
    "        driver.get(link)\n",
    "        time.sleep(random.uniform(10.75, 30.25))\n",
    "        page_to_save = driver.page_source\n",
    "        file_name = 'zillow{zc}-pg{pg}-{n}.html'.format(zc=zipcode, pg=page, n=count)\n",
    "        saved_html = open(file_name, 'w')\n",
    "        saved_html.write(page_to_save)\n",
    "        driver.back()\n",
    "    return print(count,\" total properties downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-oxford",
   "metadata": {},
   "source": [
    "### Approach #2 - Use Selenium to Click Each Individual Property on Each Search Pages and Download HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "inner-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go through search pages manually\n",
    "def get_all_html(link, zipcode):\n",
    "\n",
    "    def find_total_pages(soup):\n",
    "        total_listings = int(soup.find('div', attrs={'class' : 'total-text'}).text.replace(',', ''))\n",
    "        total_pages = total_listings//40\n",
    "        total_pages = total_pages + 2\n",
    "        return total_pages\n",
    "\n",
    "    def download_properties_html(zipcode,page):\n",
    "        property_clicks_list = driver.find_elements_by_class_name('list-card-top')\n",
    "        for count,link in enumerate(property_clicks_list, start=1):\n",
    "            time.sleep(random.uniform(20.25, 40.75))\n",
    "            if (count % random.randint(2,8) == 0):\n",
    "                time.sleep(random.uniform(30,60))\n",
    "            attempts = 0\n",
    "            while(attempts < 2):\n",
    "                try:\n",
    "                    link.find_element_by_xpath('.//a/img').click()\n",
    "                except:\n",
    "                    time.sleep(random.uniform(10,15))\n",
    "                    attempts += 1\n",
    "            time.sleep(random.uniform(10.75, 30.25))\n",
    "            page_to_save = driver.page_source\n",
    "            file_name = 'zillow{zc}-pg{pg}-{n}.html'.format(zc=zipcode, pg=page, n=count)\n",
    "            saved_html = open(file_name, 'w')\n",
    "            saved_html.write(page_to_save)\n",
    "            driver.back()\n",
    "        return print(count,\" total properties downloaded\")\n",
    "       \n",
    "    def click_next_zillow_page():\n",
    "        list_of_xpaths = ['//*[@id=\"grid-search-results\"]/div[2]/nav/ul/li[{n}]/a'.format(n=i) for i in range(1,12)]\n",
    "        for xpath in list_of_xpaths:\n",
    "            try:\n",
    "                nextpage = driver.find_element_by_xpath(xpath)\n",
    "                nextpage.click()\n",
    "            except:\n",
    "                pass\n",
    "        return print('Moved to next page')\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--user-agent=New User Agent\")\n",
    "    driver = webdriver.Chrome(chrome_path ,options=options)\n",
    "    zipcode_specific_link = link[:-1] + '-{zc}/'.format(zc=zipcode)\n",
    "    driver.get(zipcode_specific_link)\n",
    "    first_soup = BeautifulSoup(driver.page_source)\n",
    "    total_pages = find_total_pages(first_soup)\n",
    "    print('On page 1')\n",
    "    download_properties_html(zipcode,1)\n",
    "    time.sleep(random.uniform(3,6))\n",
    "    for i in range(2,total_pages):\n",
    "        if (i % random.randint(2,8) == 0):\n",
    "            time.sleep(random.uniform(30,60))\n",
    "        elif (i % random.randint(2,15) == 0):\n",
    "            time.sleep(random.uniform(300,320))\n",
    "        elif (i % random.randint(2,20) == 0):\n",
    "            time.sleep(random.uniform(400,500))\n",
    "        time.sleep(np.random.lognormal(0,1))\n",
    "        click_next_zillow_page()\n",
    "        if (i % random.randint(2,5) == 0):\n",
    "            time.sleep(random.uniform(5,10))\n",
    "        print('On page', i)\n",
    "        download_properties_html(zipcode,i)\n",
    "        time.sleep(random.uniform(3,6))\n",
    "    driver.quit()\n",
    "    return print('All complete')\n",
    "\n",
    "#Apply to all zipcodes\n",
    "def apply_all_zipcodes(philly_link,zipcode_list):\n",
    "    for zipcode in zipcode_list:\n",
    "        get_all_html(philly_link,zipcode)\n",
    "        time.sleep(random.uniform(600,800))\n",
    "    return print('Finished with all zipcodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "creative-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Chrome driver\n",
    "chrome_path = r'/Applications/chromedriver'\n",
    "\n",
    "#Link for Zillow Philadelphia region\n",
    "philly_link = 'https://www.zillow.com/philadelphia-pa/'\n",
    "\n",
    "#Pull in zipcodes\n",
    "with open('philly_zipcodes','rb') as read_file:\n",
    "    philly_zipcodes = pickle.load(read_file)\n",
    "    \n",
    "#Update based-on last zipcode successfully scrape\n",
    "zipcodes_remaining = philly_zipcodes[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "wooden-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run code\n",
    "\n",
    "apply_all_zipcodes(philly_link, zipcodes_remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-spectacular",
   "metadata": {},
   "source": [
    "### Scrape Property HTML Files - Need to Fix\n",
    "Please note, since my project moved away from Zillow scraping, this section of code is not fully complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create property dictionary\n",
    "property_headers = ['total_bedrooms', 'total_bathrooms',\n",
    "           'zip_code', 'sqft', 'property_type',\n",
    "            'year_built', 'cooling', 'heating', 'parking',\n",
    "            'lot_size_acres', 'walk_score', 'transit_score',\n",
    "            'price', 'tax_value']\n",
    "\n",
    "property_data = []\n",
    "\n",
    "property_dict = dict(zip(property_headers, ['total_bedrooms', 'total_bathrooms',\n",
    "           'zip_code', 'sqft', 'property_type',\n",
    "            'year_built', 'cooling', 'heating', 'parking',\n",
    "            'lot_size_acres', 'walk_score', 'transit_score', 'price', 'tax_value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(html_file) as page:\n",
    "        property_html = page.read()\n",
    "    property_soup = BeautifulSoup(property_html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape a single page\n",
    "\n",
    "def scrape_a_page(html_file):\n",
    "    with open(html_file) as page:\n",
    "        property_html = page.read()\n",
    "    property_soup = BeautifulSoup(property_html, \"lxml\")\n",
    "\n",
    "#Number of bedrooms/bathrooms/sqft of property\n",
    "    top_str = property_soup.find('span', attrs={'class' : 'ds-bed-bath-living-area-container'}).text\n",
    "    top_str = re.sub('[A-Za-z,]','',top_str).strip().split(' ')\n",
    "    total_bedrooms = top_str[0]\n",
    "    total_bathrooms = top_str[1]\n",
    "    sqft = top_str[2]\n",
    "\n",
    "#Zipcode\n",
    "    zip_code = property_soup.find('h1', attrs={'id' : 'ds-chip-property-address'}).text[-5:]\n",
    "\n",
    "#Property Type/year built/cooling/heating/parking\n",
    "    info_sect_str = property_soup.find('ul', attrs={'class' : 'ds-home-fact-list'}).text.replace(',','').split(':')\n",
    "    property_type = info_sect_str[1][:-10]\n",
    "    year_built = re.sub('[A-Za-z]','',info_sect_str[2])\n",
    "    cooling = info_sect_str[4][:-7]\n",
    "    heating = info_sect_str[3][:-7]\n",
    "    if property_soup.find(text=)\n",
    "    parking = info_sect_str[5][:-3]\n",
    "    lot_size_acres = info_sect_str[6][:-16]\n",
    "\n",
    "#Price\n",
    "    header_str = property_soup.find('div', attrs={'class' : 'ds-summary-row'}).text\n",
    "    total_not_needed = -len(property_soup.find('div', attrs={'class' : 'ds-bed-bath-living-area-header'}).text)\n",
    "    price = int(header_str[:total_not_needed].replace('$','').replace(',',''))\n",
    "    \n",
    "    return print('total_bedrooms:', total_bedrooms,\n",
    "                 ', total_bathrooms:', total_bathrooms,\n",
    "                ', sqft:', sqft,\n",
    "                ', zip_code:', zip_code,\n",
    "                ', property_type:', property_type,\n",
    "                ', year_built:', year_built,\n",
    "                ', cooling:', cooling,\n",
    "                ', heating:', heating,\n",
    "                ', parking:', parking,\n",
    "                ', lot_size_acres:', lot_size_acres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape individual page\n",
    "listings_without_tax_info = []\n",
    "\n",
    "if property_soup.find(text='Tax history is unavailable.') == 'NoneType':\n",
    "    \n",
    "else: \n",
    "    listings_without_tax_info.append('INSERT LINK HERE')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Walk score\n",
    "\n",
    "#Transit score\n",
    "\n",
    "#Tax value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Metis] *",
   "language": "python",
   "name": "conda-env-Metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
