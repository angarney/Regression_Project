{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "automatic-christian",
   "metadata": {},
   "source": [
    "# Zillow Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "general-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time, os\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "from fake_useragent import UserAgent\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-graph",
   "metadata": {},
   "source": [
    "### Pulling Click Individual Property Links and Download HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "creative-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Chrome driver\n",
    "chrome_path = r'/Applications/chromedriver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_philly_zipcodes = ['19104', '19131', '19139', '19143', '19151']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get scrapable pages\n",
    "def total_page_count(link,zipcode):\n",
    "    zipcode_specific_link = link[:-1] + '-{zc}/'.format(zc=zipcode)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    ua = UserAgent()\n",
    "    user_agent = ua.random\n",
    "    options.add_argument(f'user-agent={user_agent}')\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    driver = webdriver.Chrome(chrome_path,options=options)\n",
    "    driver.get(zipcode_specific_link)\n",
    "    first_soup = BeautifulSoup(driver.page_source)\n",
    "    total_listings = int(first_soup.find('div', attrs={'class' : 'total-text'}).text.replace(',', ''))\n",
    "    total_pages = total_listings//40\n",
    "    driver.quit()\n",
    "    return total_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of links\n",
    "def get_page_links(search_page):\n",
    "    for div in search_page.findAll('div', attrs={'class' : 'list-card-info'}):\n",
    "            link_list.append(div.a['href'])\n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull all links - 1st attempt\n",
    "# link_list = []\n",
    "\n",
    "# def get_all_links(link, last_page_scraped, total_pages):\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     ua = UserAgent()\n",
    "#     user_agent = ua.random\n",
    "#     options.add_argument(f'user-agent={user_agent}')\n",
    "#     options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "#     driver = webdriver.Chrome(chrome_path,options=options)\n",
    "#     for i in range(last_page_scraped,total_pages + 2):\n",
    "#         if (i % 5 == 0):\n",
    "#             options = webdriver.ChromeOptions()\n",
    "#             user_agent = ua.random\n",
    "#             options.add_argument(f'user-agent={user_agent}')\n",
    "#             options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "#             driver = webdriver.Chrome(chrome_path,options=options)\n",
    "#             time.sleep(random.uniform(300,320))\n",
    "#         else:\n",
    "#             iterable_link = link + '{pn}_p/'.format(pn=i)\n",
    "#             response = driver.get(iterable_link)\n",
    "#             page_soup = BeautifulSoup(driver.page_source)\n",
    "#             link_list = get_page_links(page_soup)\n",
    "#             print('On page', i, '& Scraped', len(link_list), 'links so far')\n",
    "#             time.sleep(random.uniform(3,6))\n",
    "#     driver.quit()\n",
    "#     return print('All complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Pull all links via zipcoes - 2nd attempt\n",
    "# link_list = []\n",
    "\n",
    "# def get_all_links(link, zipcode):\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument(\"--user-agent=New User Agent\")\n",
    "#     driver = webdriver.Chrome(chrome_path,options=options)\n",
    "#     zipcode_specific_link = link[:-1] + '-{zc}/'.format(zc=zipcode)\n",
    "#     driver.get(zipcode_specific_link)\n",
    "#     first_soup = BeautifulSoup(driver.page_source)\n",
    "#     total_listings = int(first_soup.find('div', attrs={'class' : 'total-text'}).text.replace(',', ''))\n",
    "#     total_pages = total_listings//40\n",
    "#     total_pages = total_pages + 2\n",
    "#     for i in range(1,total_pages):\n",
    "#         if (i == 1):\n",
    "#             link_list = get_page_links(first_soup)\n",
    "#             print('On page', i, '& Scraped', len(link_list), 'links so far')\n",
    "#             time.sleep(random.uniform(3,6))\n",
    "#         else:\n",
    "#             if (i % 2 == 0):\n",
    "#                 driver.quit()\n",
    "#                 options = webdriver.ChromeOptions()\n",
    "#                 user_agent = ua.random\n",
    "#                 options.add_argument(f'user-agent={user_agent}')\n",
    "#                 options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "#                 driver = webdriver.Chrome(chrome_path,options=options)\n",
    "#             if (i % 5 == 0):\n",
    "#                 time.sleep(random.uniform(300,320))\n",
    "#             iterable_link = link[:-1] + '-{zc}/'.format(zc=zipcode) + '{pn}_p/'.format(pn=i)\n",
    "#             driver.get(iterable_link)\n",
    "#             page_soup = BeautifulSoup(driver.page_source)\n",
    "#             link_list = get_page_links(page_soup)\n",
    "#             print('On page', i, '& Scraped', len(link_list), 'links so far')\n",
    "#             time.sleep(random.uniform(3,6))\n",
    "#     driver.quit()\n",
    "#     return print('All complete')\n",
    "\n",
    "#Apply to all zipcodes\n",
    "# def zipcode_apply(link,zipcode_list):\n",
    "#     for zipcode in zipcode_list:\n",
    "#         print('Starting', zipcode)\n",
    "#         get_all_links(link,zipcode)\n",
    "#     return print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Pull all links attempt #3\n",
    "# link_list = []\n",
    "\n",
    "# def get_all_links(link, zipcode):\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument(\"--user-agent=New User Agent\")\n",
    "#     driver = webdriver.Chrome(chrome_path,options=options)\n",
    "#     zipcode_specific_link = link[:-1] + '-{zc}/'.format(zc=zipcode)\n",
    "#     driver.get(zipcode_specific_link)\n",
    "#     first_soup = BeautifulSoup(driver.page_source)\n",
    "#     total_listings = int(first_soup.find('div', attrs={'class' : 'total-text'}).text.replace(',', ''))\n",
    "#     total_pages = total_listings//40\n",
    "#     total_pages = total_pages + 2\n",
    "#     link_list = get_page_links(first_soup)\n",
    "#     print('On page', 1, '& Scraped', len(link_list), 'links so far')\n",
    "#     time.sleep(random.uniform(3,6))\n",
    "#     for i in range(2,total_pages):\n",
    "#         if (i % 5 == 0):\n",
    "#             time.sleep(random.uniform(300,320))\n",
    "#         nextpage_xpth = '/html/body/div[1]/div[5]/div/div/div/div[1]/div[2]/nav/ul/li[10]/a'\n",
    "#         nextpage = driver.find_element_by_xpath(nextpage_xpth)\n",
    "#         nextpage.click()\n",
    "#         page_soup = BeautifulSoup(driver.page_source)\n",
    "#         link_list = get_page_links(page_soup)\n",
    "#         print('On page', i, '& Scraped', len(link_list), 'links so far')\n",
    "#         time.sleep(random.uniform(3,6))\n",
    "#     driver.quit()\n",
    "#     return print('All complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "inner-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save html and iterate through pages together - attempt #4\n",
    "\n",
    "def get_all_html(link, zipcode):\n",
    "\n",
    "    def find_total_pages(soup):\n",
    "        total_listings = int(soup.find('div', attrs={'class' : 'total-text'}).text.replace(',', ''))\n",
    "        total_pages = total_listings//40\n",
    "        total_pages = total_pages + 2\n",
    "        return total_pages\n",
    "\n",
    "    def download_properties_html(zipcode,page):\n",
    "        property_clicks_list = driver.find_elements_by_class_name('list-card-top')\n",
    "        for count,link in enumerate(property_clicks_list, start=1):\n",
    "            if (count % random.randint(2,8) == 0):\n",
    "                time.sleep(random.uniform(30,60))\n",
    "            attempts = 0\n",
    "            while(attempts < 2):\n",
    "                try:\n",
    "                    link.find_element_by_xpath('.//a/img').click()\n",
    "                except:\n",
    "                    time.sleep(random.uniform(10,15))\n",
    "                    attempts += 1\n",
    "            time.sleep(random.uniform(10,15))\n",
    "            page_to_save = driver.page_source\n",
    "            file_name = 'zillow{zc}-pg{pg}-{n}.html'.format(zc=zipcode, pg=page, n=count)\n",
    "            saved_html = open(file_name, 'w')\n",
    "            saved_html.write(page_to_save)\n",
    "            driver.back()\n",
    "            time.sleep(np.random.lognormal(0,1))\n",
    "        return print(count,\" total properties downloaded\")\n",
    "       \n",
    "    def click_next_zillow_page():\n",
    "        list_of_xpaths = ['//*[@id=\"grid-search-results\"]/div[2]/nav/ul/li[{n}]/a'.format(n=i) for i in range(1,12)]\n",
    "        for xpath in list_of_xpaths:\n",
    "            try:\n",
    "                nextpage = driver.find_element_by_xpath(xpath)\n",
    "                nextpage.click()\n",
    "            except:\n",
    "                pass\n",
    "        return print('Moved to next page')\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--user-agent=New User Agent\")\n",
    "    driver = webdriver.Chrome(chrome_path ,options=options)\n",
    "    zipcode_specific_link = link[:-1] + '-{zc}/'.format(zc=zipcode)\n",
    "    driver.get(zipcode_specific_link)\n",
    "    first_soup = BeautifulSoup(driver.page_source)\n",
    "    total_pages = find_total_pages(first_soup)\n",
    "    print('On page 1')\n",
    "    time.sleep(np.random.lognormal(0,1))\n",
    "    download_properties_html(zipcode,1)\n",
    "    time.sleep(random.uniform(3,6))\n",
    "    for i in range(2,total_pages):\n",
    "        if (i % random.randint(2,8) == 0):\n",
    "            time.sleep(random.uniform(30,60))\n",
    "        elif (i % random.randint(2,15) == 0):\n",
    "            time.sleep(random.uniform(300,320))\n",
    "        elif (i % random.randint(2,20) == 0):\n",
    "            time.sleep(random.uniform(400,500))\n",
    "        time.sleep(np.random.lognormal(0,1))\n",
    "        click_next_zillow_page()\n",
    "        if (i % random.randint(2,5) == 0):\n",
    "            time.sleep(random.uniform(5,10))\n",
    "        print('On page', i)\n",
    "        download_properties_html(zipcode,i)\n",
    "        time.sleep(random.uniform(3,6))\n",
    "    driver.quit()\n",
    "    return print('All complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "consistent-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipcode = '19102'\n",
    "# link = philly_link\n",
    "# driver = webdriver.Chrome(chrome_path)\n",
    "# zipcode_specific_link = link[:-1] + '-{zc}/'.format(zc=zipcode)\n",
    "# driver.get(zipcode_specific_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "isolated-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wooden-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Link for Zillow Philadelphia region\n",
    "philly_link = 'https://www.zillow.com/philadelphia-pa/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ahead-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull in zipcodes\n",
    "\n",
    "with open('philly_zipcodes','rb') as read_file:\n",
    "    philly_zipcodes = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cathedral-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply to all zipcodes\n",
    "\n",
    "def apply_all_zipcodes(philly_link,zipcode_list):\n",
    "    for zipcode in zipcode_list:\n",
    "        get_all_html(philly_link,zipcode)\n",
    "        time.sleep(random.uniform(600,800))\n",
    "    return print('Finished with all zipcodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "intellectual-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodes_remaining = philly_zipcodes[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "greek-wallet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On page 1\n",
      "40  total properties downloaded\n",
      "Moved to next page\n",
      "On page 2\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'count' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-fd88f3aae79e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Run code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mapply_all_zipcodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphilly_link\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzipcodes_remaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-cc565d99f06c>\u001b[0m in \u001b[0;36mapply_all_zipcodes\u001b[0;34m(philly_link, zipcode_list)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_all_zipcodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphilly_link\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzipcode_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mzipcode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzipcode_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mget_all_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphilly_link\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzipcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished with all zipcodes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-b85b512b3a6b>\u001b[0m in \u001b[0;36mget_all_html\u001b[0;34m(link, zipcode)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'On page'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mdownload_properties_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-b85b512b3a6b>\u001b[0m in \u001b[0;36mdownload_properties_html\u001b[0;34m(zipcode, page)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlognormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" total properties downloaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclick_next_zillow_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'count' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#Run code\n",
    "apply_all_zipcodes(philly_link, zipcodes_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle final list\n",
    "with open('philly_property_lists_19103', 'wb') as philly_links:\n",
    "    pickle.dump(link_list, philly_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-spectacular",
   "metadata": {},
   "source": [
    "### Scrape Property HTML Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create property dictionary\n",
    "property_headers = ['total_bedrooms', 'total_bathrooms',\n",
    "           'zip_code', 'sqft', 'property_type',\n",
    "            'year_built', 'cooling', 'heating', 'parking',\n",
    "            'lot_size_acres', 'walk_score', 'transit_score',\n",
    "            'price', 'tax_value']\n",
    "\n",
    "property_data = []\n",
    "\n",
    "property_dict = dict(zip(property_headers, ['total_bedrooms', 'total_bathrooms',\n",
    "           'zip_code', 'sqft', 'property_type',\n",
    "            'year_built', 'cooling', 'heating', 'parking',\n",
    "            'lot_size_acres', 'walk_score', 'transit_score', 'price', 'tax_value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_a_page('prop1.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(html_file) as page:\n",
    "        property_html = page.read()\n",
    "    property_soup = BeautifulSoup(property_html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape a single page\n",
    "\n",
    "def scrape_a_page(html_file):\n",
    "    with open(html_file) as page:\n",
    "        property_html = page.read()\n",
    "    property_soup = BeautifulSoup(property_html, \"lxml\")\n",
    "\n",
    "#Number of bedrooms/bathrooms/sqft of property\n",
    "    top_str = property_soup.find('span', attrs={'class' : 'ds-bed-bath-living-area-container'}).text\n",
    "    top_str = re.sub('[A-Za-z,]','',top_str).strip().split(' ')\n",
    "    total_bedrooms = top_str[0]\n",
    "    total_bathrooms = top_str[1]\n",
    "    sqft = top_str[2]\n",
    "\n",
    "#Zipcode\n",
    "    zip_code = property_soup.find('h1', attrs={'id' : 'ds-chip-property-address'}).text[-5:]\n",
    "\n",
    "#Property Type/year built/cooling/heating/parking\n",
    "    info_sect_str = property_soup.find('ul', attrs={'class' : 'ds-home-fact-list'}).text.replace(',','').split(':')\n",
    "    property_type = info_sect_str[1][:-10]\n",
    "    year_built = re.sub('[A-Za-z]','',info_sect_str[2])\n",
    "    cooling = info_sect_str[4][:-7]\n",
    "    heating = info_sect_str[3][:-7]\n",
    "    if property_soup.find(text=)\n",
    "    parking = info_sect_str[5][:-3]\n",
    "    lot_size_acres = info_sect_str[6][:-16]\n",
    "\n",
    "#Price\n",
    "    header_str = property_soup.find('div', attrs={'class' : 'ds-summary-row'}).text\n",
    "    total_not_needed = -len(property_soup.find('div', attrs={'class' : 'ds-bed-bath-living-area-header'}).text)\n",
    "    price = int(header_str[:total_not_needed].replace('$','').replace(',',''))\n",
    "    \n",
    "    return print('total_bedrooms:', total_bedrooms,\n",
    "                 ', total_bathrooms:', total_bathrooms,\n",
    "                ', sqft:', sqft,\n",
    "                ', zip_code:', zip_code,\n",
    "                ', property_type:', property_type,\n",
    "                ', year_built:', year_built,\n",
    "                ', cooling:', cooling,\n",
    "                ', heating:', heating,\n",
    "                ', parking:', parking,\n",
    "                ', lot_size_acres:', lot_size_acres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape individual page\n",
    "listings_without_tax_info = []\n",
    "\n",
    "if property_soup.find(text='Tax history is unavailable.') == 'NoneType':\n",
    "    \n",
    "else: \n",
    "    listings_without_tax_info.append('INSERT LINK HERE')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Walk score\n",
    "\n",
    "#Transit score\n",
    "\n",
    "#Tax value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Metis] *",
   "language": "python",
   "name": "conda-env-Metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
